[{"authors":["admin"],"categories":null,"content":"Update: I will start as a PhD student at Waterloo CS in Jan, 2021.\nMinghan Li is a MSc student in Computer Science Department at Universtiy of Toronto. He is interested in Natural Language Understanding, Information Retrieval, and Question Answering. He also has broad interests in Reinforcement Learning (RL), especially task-agnostic RL and model-based RL.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://alexlimh.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Update: I will start as a PhD student at Waterloo CS in Jan, 2021.\nMinghan Li is a MSc student in Computer Science Department at Universtiy of Toronto. He is interested in Natural Language Understanding, Information Retrieval, and Question Answering. He also has broad interests in Reinforcement Learning (RL), especially task-agnostic RL and model-based RL.","tags":null,"title":"Minghan Li","type":"authors"},{"authors":["**Minghan Li**\u0026ast;","He Bai\u0026ast;","Luchen Tan","Kun Xiong","Ming Li","Jimmy Lin"],"categories":null,"content":"","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603238400,"objectID":"743a7879f2d392fd65aacde3129442bf","permalink":"https://alexlimh.github.io/publication/pwa/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/publication/pwa/","section":"publication","summary":"*Preprint, arXiv*, 2020","tags":null,"title":"Latte-Mix: Measuring Sentence Semantic Similarity with Latent Categorical Mixtures","type":"publication"},{"authors":["**Minghan Li**\u0026ast;","Tanli Zuo\u0026ast;","Ruicheng Li","Martha White","Weishi Zheng"],"categories":null,"content":"","date":1543795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543795200,"objectID":"b5404f0936aa95942a13ab5146128ba1","permalink":"https://alexlimh.github.io/publication/distillation/","publishdate":"2018-12-03T00:00:00Z","relpermalink":"/publication/distillation/","section":"publication","summary":"*Preprint, arXiv*, 2018","tags":null,"title":"Accelerating Large Scale Knowledge Distillation via Dynamic Importance Sampling","type":"publication"}]